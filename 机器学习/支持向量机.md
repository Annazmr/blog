# 支持向量机

</br>

#### 什么是支持向量机

- 支持向量机（Support Vector Machines, SVM）是一种**二分类模型**。它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的**非线性分类器**。
- SVM的学习策略就是间隔最大化，可形式化为一个**求解凸二次规划的问题**，也等价于正则化的合页损失函数的最小化问题。
- SVM的最优化算法是求解凸二次规划的最优化算法。
- 对于训练集$T=\{(x_1,y_1),\ldots,(x_N,y_N)\}$，标签$y_i\in\{+1,-1\}$，支持向量机期望寻找分类超平面$w^Tx+b=0$。

> [!TIP|label:什么是支持向量]
> 训练数据集中**与分离超平面距离最近的样本点**的实例称为支持向量。

</br>

#### SVM的分类

- **线性可分支持向量机**（硬间隔支持向量机）</br>
  当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
- **线性支持向量机**（软间隔支持向量机）</br>
  当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
- **非线性支持向量机**</br>
  当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

</br>

#### 函数间隔和几何间隔

- 一个点距离分离超平面的远近可以表示分类预测的**确信程度**。
- 在分类超平面确定的情况下，$|wx+b|$能够相对地表示点$x$距离超平面的远近。而$w^Tx+b$与标签$y$的符号是否一致能够表示分类是否正确。因此**函数间隔表示了分类的正确性和确信度**
  $$
  {\hat{\gamma}}_i=y_i(w^Tx_i+b).
  $$
  而超平面$(w,b)$关于某一训练集$T$的函数间隔指的是所有样本点中函数间隔的最小值
  $$
  \hat{\gamma}=\min{{\hat{\gamma}}_i}.
  $$
- 如果对超平面的法向量$w$增加约束，**使得间隔不随超平面的缩放而变化，引入几何间隔**
  $$
  \color{red} \gamma_i=\dfrac{y_i}{\|w\|}(w^Tx_i+b)=\dfrac{\gamma_i}{\|w\|}.
  $$

</br>

#### 为什么要采用间隔最大化

- 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。利用间隔最大化求得**最优分离超平面**，这时，**解是唯一的**。另一方面，此时的分隔超平面所产生的分类结果对未知实例的**泛化能力最强**。
- 不仅要将正负样例点分开，而且对于最难分的点也要有足够大的确信度把它们分开。

</br>

#### 最大间隔分离超平面
- 最大间隔分类超平面中的“间隔”指的是**几何间隔**。
- 目标函数可以定义为$\max{\gamma}$，可以导出为一个**约束最优化问题**
  $$
  \begin{aligned}
  &\max_{w,b}\gamma \\
  &{\rm s.t.}\ \ y_i\dfrac{f(x_i)}{\|w\|}\ge\gamma,\ i=1,\ldots,N.
  \end{aligned}
  $$
- 根据几何间隔和函数间隔的关系$\gamma=\dfrac{\hat{\gamma}}{\|w\|}$，若令函数间隔$\hat{\gamma}=1$（方便推导和优化），可以将问题改写为
  $$
  \begin{aligned}
  &\max_{w,b}\dfrac{1}{\|w\|}\\
  &{\rm s.t.}\ \ y_i(w^Tx_i+b)\ge1,\ i=1,\ldots,N.
  \end{aligned}
  $$
- 由于求解$\dfrac{1}{\|w\|}$的最大值相当于求解$\dfrac{1}{2}{\|w\|}^2$的最小值，上述问题等价于
  $$
  \begin{aligned}
  &\color{red} \min_{w,b}\dfrac{1}{2}{\|w\|}^2 \\
  &\color{red} {\rm s.t.}\ \ y_i(w^Tx_i+b)-1\ge0,\ i=1,\ldots,N.
  \end{aligned}
  $$
  由于目标函数是二次的，约束条件是线性的，所以它是一个**凸二次规划问题**，即约束最优化问题。
- 线性可分训练数据集的最大间隔分离超平面是**存在且唯一**的。

</br>

#### 对偶问题

- 约束最优化问题可以利用**拉格朗日对偶性**通过变换得到对偶变量的优化问题，即通过求解与原问题等价的对偶问题得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法。
- 简单来讲，通过给每一个约束条件加上一个拉格朗日乘子$\alpha_i$（Lagrange multiplier），定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出问题）
  $$
  \color{red} L(w,b,\alpha)=\frac{1}{2}{\|w\|}^2-\sum_{i=1}^{N}{\alpha_i\left(y_i(w^Tx_i+b)-1\right)}.
  $$
  问题的目的是寻找$\alpha_i\ge0$使得目标函数尽可能大，若存在不满足$y_i(w^Tx_i+b)-1\ge0$的情况，只需要将$\alpha_i$取无穷大即可最大化$L(w,b,\alpha)$。在要求约束条件得到满足的情况下最小化$\frac{1}{2}{\|w\|}^2$，实际上等价于最小化$\max_\alpha L(w,b,\alpha)$
  $$
  \min_{w,b}\max_\alpha L(w,b,\alpha).
  $$
  根据最小化根据拉格朗日对偶性，原始问题的**对偶问题**是极大极小问题
  $$
  \max_\alpha\min_{w,b} L(w,b,\alpha).
  $$
- 原始问题和对偶问题等价需要满足**KKT条件**，它是一个非线性规划问题能有最优化解法的必要和充分条件。

</br>

#### 为什么要引入对偶问题

- 对偶问题往往**更容易求解**</br>
  当寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。
- 目前处理的模型**严重依赖于数据集的维度`d`**，如果维度`d`太高就会严重提升运算时间；对偶问题把SVM从依赖`d`个维度转变到依赖`N`个数据点，最后计算时只有支持向量有意义，所以计算量比`N`小很多。
- 方便**引入核函数**，进而推广到非线性分类问题。

</br>

#### 求解对偶问题

- 固定$\alpha$，要让目标函数关于$w$和$b$最小化，对两者分别**求偏导并令偏导为零**
  $$
  \begin{aligned}
  &\partial_w L=w-\sum_{i=1}^{N}{\alpha_iy_ix_i}=0 \to w=\sum_{i=1}^{N}{\alpha_iy_ix_i},\\
  &\partial_b L=-\sum_{i=1}^{N}{\alpha_iy_i}=0.
  \end{aligned}
  $$
  把结果代回原式
  $$
  \begin{aligned}
  L(w,b,\alpha)&=\dfrac{1}{2}w^Tw-\sum_{i=1}^{N}{\alpha_iy_iw^Tx_i}-\sum_{i=1}^{N}{\alpha_iy_ib}+\sum_{i=1}^{N}{\alpha_i} \\
  &=-\dfrac{1}{2}{w^T}\sum_{i=1}^{N}{\alpha_iy_ix_i}-b\sum_{i=1}^{N}{\alpha_iy_i}+\sum_{i=1}^{N}{\alpha_i} \\
  &=\sum_{i=1}^{N}\alpha_i-\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}.
  \end{aligned}
  $$
- 上一步得到的$L(w,b,\alpha)$实际已经只包含一个变量$\alpha$，求对$\alpha$的极大，也就是一个关于对偶问题的最优化问题
  $$
  \begin{aligned}
  &\max_\alpha{\sum_{i=1}^{N}\alpha_i-\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}} \\
  &{\rm s.t.}\ \ \sum_{i=1}^{N}{\alpha_iy_i}=0\ \&\ \alpha_i\ge0,\ i=1,2,\ldots,N.
  \end{aligned}
  $$
  转化为对偶问题（约束最优化问题）
  $$
  \color{red} \min_\alpha{\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jx_i^Tx_j}-\sum_{i=1}^{N}\alpha_i},
  $$
  这样就可以计算出最优解$\alpha^\ast=\left(\alpha_1^\ast,\alpha_2^\ast,\ldots,\alpha_N^\ast\right)^T$。
- 从而计算其它参数
  $$
  w^\ast=\sum_{i=1}^{N}{\alpha_i^\ast y_ix_i},
  $$
  可以选择某一个正分量$\alpha_j^\ast>0$，计算
  $$
  b^\ast=y_j-\sum_{i=1}^{N}{\alpha_i^\ast y_i\left(x_i^Tx_j\right)}.
  $$
  从而得到分离超平面${w^\ast}^Tx+b^\ast=0$，以及分类决策函数$f(x)={\rm sign}({w^\ast}^Tx+b^\ast)$。

</br>

#### 线性支持向量机和软间隔最大化

- 对于线性不可分的数据集，意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于`1`的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量$\xi_i\geq0$，使函数间隔加上**松弛变量**大于等于`1`。 这样，约束条件变为
  $$
  y_i(w^Tx_i+b)\geq1-\xi_i.
  $$
  同时，对于每个松弛变量$\xi_x$，需要支付一个代价$\xi_x$，使得目标函数由原来的$\dfrac{1}{2}{\|w\|}^2$变成
  $$
  \dfrac{1}{2}{\|w\|}^2+C\sum_{i=1}^{N}\xi_i,
  $$
  其中，惩罚参数$C>0$，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。相对于硬间隔最大化，这种情况被称为**软间隔最大化**。
- 线性不可分的线性支持向量机的学习问题变成如下的凸二次规划问题
  $$
  \begin{aligned}
  &\min_{w,b,\xi} \dfrac{1}{2}{\|w\|}^2+C\sum_{i=1}^{N}\xi_i \\
  &{\rm s.t.}\ \ y_i(w^T x_i+b)\geq 1-\xi_i,\ \xi_i\geq 0,\ i=1,2,\ldots,N.
  \end{aligned}
  $$

</br>

#### 加入松弛变量的SVM的训练误差可以为0吗

- 使用SMO算法训练的线性分类器并**不一定**能得到训练误差为`0`的模型。这是由于优化目标改变了，并不再是使训练误差最小。
- 考虑带松弛变量的SVM模型优化的目标函数所包含的两项$\dfrac{1}{2}{\|w\|}^2$和$C\sum_{i=1}^{N}\xi_i$，当参数$C$选取较小的值时，后一项（正则项）将占据优化的较大比重。这样，一个带有训练误差，但是参数较小的点将成为更优的结果。一个简单的特例是，当$C$取`0`时，$w$也取`0`即可达到优化目标，但是显然此时的训练误差不一定能达到`0`。

</br>

#### 合页损失函数

上述的凸二次规划问题$\min_{w,b,\xi}$等价于最优化问题
$$
\color{red} \min{w,b}\sum_{i=1}^{N}\left[1-y_i(w^T x_i+b)\right]_+ +\lambda{\|w\|}^2.
$$
**合页损失函数**为
$$
\color{red} L(y(w^Tx+b))=\left[1-y(w^Tx+b)\right]_+,
$$
其中
$$
[z]_+=\left\{
\begin{aligned}
z, &z>0 \\
0, &z\le0
\end{aligned}
\right..
$$

</br>

#### 非线性支持向量机与核函数

- 对于非线性问题，希望采取一个**非线性变换（映射）**，将其转换为线性问题。SVM 的处理方法是选择一个**核函数**，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。这意味着建立非线性学习器分为两步：
  1. 首先使用一个非线性映射将数据变换到一个特征空间`F`；
  2. 然后在特征空间使用线性学习器分类。
- 设$\mathcal{X}$是输入空间（欧氏空间的子集或离散集合），设$\mathcal{H}$为特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射$\emptyset(x):\mathcal{X}\rightarrow\mathcal{H}$，使得对于所有$x,z\in\mathcal{X}$，函数$K(x,z)=\emptyset(x)\cdot\emptyset(z)$，则称$K(x,z)$为核函数，$\emptyset(x)$为映射函数，$\emptyset(x)\cdot\emptyset(z)$表示两者的内积。
- 通常把计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数，核函数能简化映射空间中的内积运算，碰巧SVM 里需要计算的地方数据向量总是以内积的形式出现。
- 此时可以将**对偶问题的目标函数**改写为
  $$
  w(\alpha)=\dfrac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha_i\alpha_jy_iy_jK(x_i,x_j)}-\sum_{i=1}^{N}\alpha_i.
  $$
  分类决策函数中的内积也可以用核函数代替
  $$
  f(x)=sign\left(\sum_{i=1}^{N_s}{\alpha_i^\ast y_iK\left(x_i,x\right)+b^\ast}\right).
  $$

</br>

#### 常见的核函数

- 线性核函数（无核）$K(x,z)=x\cdot z$。
- 多项式核函数$K(x,z)=(x\cdot z+1)^p$。
- 高斯核函数（RBF核函数）$K(x,z)=\exp\left(-\dfrac{{\|x-z\|^2}}{2\sigma^2}\right)$。**高斯核可以将原始空间映射为无穷维空间**，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。

</br>

#### 核函数的选择

- 最常用的是Linear核与RBF核（高斯核）。至于到底该采用哪种核，**要根据具体问题**，有的数据是线性可分的，有的不可分，需要多尝试不同核不同参数。
- Linear核</br>
  **主要用于线性可分的情形。**参数少，速度快，对于一般数据，分类效果已经很理想了。
- RBF核</br>
  **主要用于线性不可分的情形。**参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。

</br>

#### RBF核的优点

- RBF核函数可以将一个样本映射到一个更高维的空间，而且线性核函数是RBF的一个特例，也就是说如果考虑使用RBF，那么就没有必要考虑线性核函数了。
- 与多项式核函数相比，RBF需要确定的参数要少，核函数参数的多少直接影响函数的复杂程度。另外，当多项式的阶数比较高时，核矩阵的元素值将趋于无穷大或无穷小，而RBF则在上，会减少数值的计算困难。
- 对于某些参数，RBF和Sigmoid具有相似的性能。

</br>

#### 为什么SVM对缺失数据敏感

缺失数据是指缺失某些特征数据，**向量数据不完整**。**SVM没有处理缺失值的策略**（决策树有），SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。

</br>

#### 样本失衡会对SVM的结果产生影响吗

- 会，**超平面会靠近样本少的类别**。因为使用的是软间隔分类，而如果对所有类别都是使用同样的惩罚系数，则由于优化目标里面有最小化惩罚量，所以靠近少数样本时，其惩罚量会少一些。
- 解决方法
  1. 对多数类和和少数类采用不同的惩罚因子，例如正例远少于负例，则正例的`C`值取得较大。这种方法的缺点是可能会偏离原始数据的概率分布；
  2. 对训练集的数据进行预处理即对数量少的样本以某种策略进行采样，增加其数量或者减少数量多的样本，典型的方法如：随机插入法。缺点是可能出现overfitting；
  3. 基于核函数的不平衡数据处理。

</br>

#### LR和SVM的区别和联系

- 相同
  1. 都是**有监督分类方法**，判别模型（直接估计$y=f(x)$或$p(y|x)$）；
  2. 都是**线性分类方法**（指不用核函数的情况）
- 不同
  1. 损失函数不同（交叉熵损失和合页损失），最大化函数间隔；
  2. LR决策考虑所有样本点，SVM决策仅仅取决于支持向量；
  3. LR受数据分布影响，尤其是样本不均衡时影响大，要先做平衡，SVM不直接依赖于分布；
  4. LR可以产生概率，SVM不能；
  5. LR不依赖样本之间的距离，SVM是基于距离的；
  6. LR是经验风险最小化，需要另外加正则，SVM自带结构风险最小化不需要加正则项；
  7. 在解决非线性问题时，SVM 采用核函数的机制，而 LR 通常不采用核函数的方法；
  8. 小数据集SVM比LR好，大数据SVM计算复杂，LR简单且能够在线并行训练，因此更多用LR。

</br>

> [!NOTE|label:参考资料]
> [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_JULY_v/article/details/7624837#commentBox)</br>
> [SVM之面试常问问题](https://blog.csdn.net/Jum_Summer/article/details/80793835)
>