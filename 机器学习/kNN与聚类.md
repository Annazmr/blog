# kNN与聚类

</br>

#### k近邻法

- k近邻法（k-Nearest Neighbor, kNN）是一种基本**分类与回归方法**，而不是聚类算法。
- 其算法基本思想是，给定测试实例，基于某种**距离度量**找出训练集中与其**最靠近的`k`个实例点**，然后基于这`k`个最近邻的信息来进行预测。
- 流程描述：
  1. 计算**测试数据**与各个**训练数据**之间的距离；
  2. 按照距离的**递增**关系进行排序；
  3. 选取**距离最小**的`k`个点；
  4. 确定这`k`个点所在类别的**出现频率**；
  5. 返回这`k`个点中出现频率**最高**的类别作为测试数据的预测分类。
- **三个基本要素：**k值的选择、距离度量、分类决策规则。
- 在分类任务中可使用**投票法**，即选择这`k`个实例中出现最多的标记类别作为预测结果；</br>
  在回归任务中可使用**平均法**，即将这`k`个实例的实值输出标记的平均值作为预测结果；</br>
  还可基于距离远近进行**加权平均或加权投票**，距离越近的实例权重越大。

<div align=center><img src="./img/kNN.png" width=300px/></div>

</br>

#### kNN中的距离度量

- 特征空间中的**两个实例点的距离是两个实例点相似程度的反映**。k近邻法的特征空间一般是`n`维实数向量空间，使用的距离是欧氏距离。
- 定义向量$x_i=\left(x_i^{(1)},\ldots,x_i^{(n)}\right)^T$和$x_j=\left(x_j^{(1)},\ldots,x_j^{(n)}\right)^T$，向量间的闵可夫斯基距离$L_p$距离定义为
  $$
  L_p(x_i,x_j)=\left(\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|^p\right)^\frac{1}{p},
  $$
  其中$p\geq1$。
  1. 当$p=1$时，称为**曼哈顿距离**
    $$
    L_1(x_i,x_j)=\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|.
    $$
  2. 当$p=2$时，称为**欧氏距离**
    $$
    \color{red} L_2(x_i,x_j)=\left(\sum_{l=1}^n\left|x_i^{l}-x_j^{l}\right|^2\right)^\frac{1}{2}.
    $$
  3. 当$p=3$时，为**各个坐标距离的最大值**
    $$
    L_\infty(x_i,x_j)=\max_l\left|x_i^{l}-x_j^{l}\right|.
    $$

</br>

#### kNN中`k`的取值

- 如果`k`的取值过小时，一旦有噪声得成分存在将会对预测产生比较大影响，例如取`k`值为`1`时，一旦最近的一个点是噪声，那么就会出现偏差。`k`值的减小就意味着整体模型变得复杂，容易发生过拟合。
- 如果`k`的取值过大时，就相当于用较大邻域中的训练实例进行预测，学习的近似误差会增大。这时与输入目标点较远实例也会对预测起作用，使预测发生错误。`k`值的增大就意味着整体的模型变得简单。
- 常用的方法是从`k = 1`开始，使用**验证集**估计分类器的误差率。重复该过程，每次`k`增值`1`，允许增加一个近邻。选取产生最小误差率的`k`。
- 从经验上看，一般`k`的取值不超过`20`，**上限是`n`的开方**。随着数据集的增大，`k`的值也要增大。

</br>

#### k-d树

- kNN方法对于每个新的输入实例都需要与所有的训练实例计算一次距离并排序。当训练集非常大的时候，计算就非常耗时、耗内存，导致算法的效率降低。k-d树（k-dimensional tree）是一个可以**减少计算距离次数**的方法，构建是一个递归过程，复杂度为`O(logN)`。
- 其算法基本思想是，计算所有数据点在各个维度上的方差，取**方差最大的维度中的数据中值来划分左右子空间**，直到所有数据点都落在一个节点上。
- 假设`6`个二维数据点`{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)}`，构建k-d树的步骤为：
  1. **确定split域**，数据点在`x`和`y`维度上的数据方差分别为`39`、`28.63`，所以选择维度`x`作为split域；
  2. **确定Node-data的值**，根据维度`x`上所有当前数据点排序后的中值`7`，选择Node-data数据点`(7, 2)`。因此，该节点的分割超平面就是直线`x = 7`；
  3. **确定左右子空间**，根据`x = 7`将$x\leq7$和$x>7$分别划分为左右子空间，其中左子空间包含数据点`{(2, 3), (5, 4), (4, 7)}`，右子空间包含剩下的两个数据点；
  4. 重复对左右子空间执行上述操作。
  得到的空间划分和k-d树如下图所示。

<div align=center><img src="./img/k-d树.jpg" width=550px/></div>

- 在搜索一个新的数据点`S`的时候，通过二叉搜索递归向下，找到一个可能的最近邻点`A`（叶子节点），并以`S`为圆心，`S`和`A`的距离为半径作圆。如果该圆与`A`的父节点`P`的划分线（分割超平面）相交，则需要搜索`P`及`P`的另一个子空间，寻找新的最近邻点。不断回溯直到根节点，即可找到`S`的最近邻点。

</br>

#### K-Means聚类算法

- K-Means算法又名K均值算法，是一个**无监督的聚类算法**，试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“**簇**”（cluster）。过这样的划分，每个簇可能对应于一些潜在的概念或类别。
- 其算法基本思想是：通过不断调整`K`个簇的簇中心位置，**让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大**。
- 流程描述：
  1. 先从样本集中**随机选取`K`个样本**$\{\mu_1,\ldots,\mu_K\}$作为簇中心（质心向量）；
  2. 计算每一个样本$x_i$与这`K`个簇中心$\mu_j$的距离${\|x_i-\mu_j\|}_2^2$，将其划分到**与其距离最近的簇中心**所在的簇（类别）中；
  3. 对于每一个簇，**计算新的簇中心**$\hat{\mu_j}=\dfrac{1}{N_j}\sum x_i$；
  4. 如果所有簇中心都不再发生变化，或满足最大运行轮数或最小调整幅度阈值，结束。

<div align=center><img src="./img/K-Means.jpeg" width=350px/></div>

</br>

#### K-Means中`K`的取值

- **手肘法**
  - 核心指标是**误差平方和**SSE（Sum of the Squared Errors）
    $$
    {\rm SSE}=\sum_{i=1}^k\sum_{x\in C_i}{|x-\mu_i|}^2
    $$
    其中$C_i$是第`i`个簇，$x$是$C_i$中的样本点，$\mu_i$是簇中心（质心/均值向量）。SSE可以衡量聚类误差。
  - 核心思想是：随着聚类数`K`的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。在这个过程中，当`K`小于真实聚类数时，**SSE的下降幅度会很大**，而当`K`到达真实聚类数时，再增加`K`所得到的聚合程度回报会迅速变小，所以**SSE的下降幅度会骤减**，进而趋于平缓，也就是说SSE和`K`的关系图是一个手肘的形状，而这个肘部对应的`K`值就是数据的真实聚类数。

<div align=center><img src="./img/手肘法.png" width=400px/></div>

- **轮廓系数**
  - 核心指标是**轮廓系数**（Silhouette Coefficient），某个样本点$x_i$的轮廓系数定义如下：
    $$
    S=\dfrac{b-a}{\max(a,b)}
    $$
    其中，$a$是$x_i$与**同簇的其它样本的平均距离**（凝聚度），$x_i$与其它簇的距离表示为与该簇中所有样本的平均距离，其中$b$表示$x_i$**与最近的一个簇的距离**（分离度）。
  - 平均轮廓系数的取值范围为`[-1, 1]`，且**簇内样本的距离越近，簇间样本距离越远**，平均轮廓系数越大，聚类效果越好。那么，平均轮廓系数最大的`K`便是最佳聚类数。

</br>

#### K-Means的优点和缺点

- 优点：
  1. 原理比较简单，实现也是很容易，收敛速度快；
  2. 聚类效果较优；
  3. 算法的**可解释度比较强**；
  4. 主要需要调参的参数仅仅是簇数`K`。
- 缺点：
  1. **`K`值的选取不好把握**；
  2. 对于非凸的数据集比较难收敛；
  3. 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
  4. 采用迭代方法，得到的结果只是局部最优。
  5. **对噪音和异常点比较的敏感。**

</br>

#### K-Means中空聚类的处理

- 选择一个**距离当前任何质心最远的点**，这将消除当前对总平方误差影响最大的点。
- 从具有**最大SSE的簇中**选择一个**替补的质心**，这将分裂簇并降低聚类的总SSE。如果有多个空簇，则该过程重复多次。
- 如果噪点或者孤立点过多，考虑更换算法，如密度聚类。

</br>

#### K-Means++算法

- 由于K-Means算法的分类结果会**受到初始点的选取而有所区别**，因此K-Means++是一种改进算法。它仅仅改变了初始聚类中心（质心）的选择，基本思路就是使初始的质心之间的**相互距离要尽可能的远**。
- 流程描述：
  1. 随机选取一个样本点作为第一个聚类中心；
  2. 计算每一个样本与当前所有聚类中心的**最短距离**（即与最近一个聚类中心的距离），选择一个距离最大的点作为又一个聚类中心；
  3. 重复直至选出`K`个聚类中心；
  4. 利用这`K`个质心来作为初始化质心去运行标准的K-Means算法。

</br>

#### DBSCAN聚类算法

- DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种**基于密度的空间聚类算法**。该算法将**具有足够密度的区域**划分为簇，并在具有噪声的空间数据库中发现**任意形状的簇**，它将簇定义为密度相连的点的最大集合。

> [!TIP]
> 形象来说，算法随机选中一个样本点，围绕这个被选中的样本点**画一个圆**，规定这个**圆的半径**以及**圆内最少包含的样本点**，如果在指定半径内有足够多的样本点在内，那么这个圆圈的圆心就转移到这个内部样本点，继续去圈附近其它的样本点。等到这个滚来滚去的圈发现所圈住的样本点无法满足条件，就停止了。那么称最开始那个点为**核心点**，停下来的那个点为**边界点**，没有被包含的点为**离群点**。

- 密度定义：
  1. **$\epsilon$-邻域：**对于$x_j\in D$，其$\epsilon$-邻域包含样本集$D$中与$x_j$的**距离不大于$\epsilon$的子样本集**，即$N_{\epsilon(x_j)}={x_i\in D|{\rm distance}(x_i,x_j)\le\epsilon}$, 这个子样本集的个数记为$|N_{\epsilon(x_j)}|$。
  2. **核心对象：**对于任一样本$x_j\in D$，如果其$\epsilon$-邻域对应的$|N_{\epsilon(x_j)}|$至少包含`MinPts`个样本，则$x_j$是核心对象。
  3. **密度直达：**如果$x_i$位于$x_j$的$\epsilon$-邻域中，且$x_j$是核心对象，则称$x_i$由$x_j$密度直达。注意反之不一定成立。
  4. **密度可达：**对于$x_i$和$x_j$，如果$x_i$通过多次密度直达可以到达$x_j$，则称$x_j$由$x_i$密度可达。也就是说，密度可达满足传递性，但也不满足对称性。
  5. **密度相连：**对于$x_i$和$x_j$,如果存在核心对象样本$x_k$，使$x_i$和$x_j$均由$x_k$密度可达，则称$x_i$和$x_j$密度相连。注意密度相连关系是满足对称性的。

</br>

#### DBSCAN的优缺点

- 优点：
  1. 可以对**任意形状的稠密数据集**进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集；
  2. 可以在聚类的同时发现异常点，对数据集中的**异常点不敏感**；
  3. 聚类结果**没有偏倚**，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。
- 缺点：
  1. 如果样本集的**密度不均匀、聚类间距差相差很大**时，聚类质量较差，这时用DBSCAN聚类一般不适合；
  2. 如果**样本集较大**时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
  3. 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对**距离阈值**$\epsilon$、**邻域样本数阈值**`MinPts`联合调参，不同的参数组合对最后的聚类效果有较大影响。

</br>

> [!NOTE|label:参考资料]
> [机器学习（二）：k近邻法（kNN）](https://blog.csdn.net/eeeee123456/article/details/79927128)</br>
> [kNN算法：K最近邻分类算法](https://www.cnblogs.com/jyroy/p/9427977.html)</br>
> [机器学习系列之——Knn算法 kd树详解](https://cloud.tencent.com/developer/news/212042)</br>
> [K-Means聚类算法原理](https://www.cnblogs.com/pinard/p/6164214.html)</br>
> [K-means聚类最优k值的选取](https://blog.csdn.net/qq_15738501/article/details/79036255)</br>
> [K-means与K-means++](https://www.cnblogs.com/wang2825/articles/8696830.html)</br>
> [聚类（二）——KMeans算法](https://blog.csdn.net/WangZixuan1111/article/details/98970139)</br>
> [DBSCAN聚类算法——机器学习](https://blog.csdn.net/huacha__/article/details/81094891)</br>
> [DBSCAN密度聚类算法](https://www.cnblogs.com/zhengxingpeng/p/6670486.html)
