# 集成学习

</br>

#### 基本思想

通过**构建并结合多个学习器**来完成学习任务，组合成一个性能更好的学习器，有时也叫多分类器系统。

> [!TIP|label:集成学习为什么有效？]
> 不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且如果成员的误差是独立的，集成模型将显著地比其成员表现更好。

</br>

#### 学习策略

- 对于训练集数据，通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。也就是说，集成学习有两个主要的问题需要解决，第一是**如何得到若干个个体学习器**，第二是**如何选择一种结合策略**，将这些个体学习器集合成一个强学习器。
- 第一种就是所有的个体学习器都是一个种类的，或者说是**同质**的。比如都是决策树或者神经网络个体学习器。
- 第二种是所有的个体学习器不全是一个种类的，或者说是**异质**的。
- 同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类：
  1. 个体学习器之间**存在强依赖关系**，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法；
  2. 个体学习器之间**不存在强依赖关系**，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。

> [!NOTE]
> 目前来说，同质个体学习器的应用是最广泛的，一般常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。

</br>

#### 集成学习有哪些基本步骤

集成学习一般可分为以下3个步骤：
1. 找到误差相互独立的基分类器；
2. 训练基分类器；
3. 合并基分类器的结果。

</br>

#### 常见的结合策略

- 平均法
  1. 对于数值类**回归预测问题**，最常见的结合策略是平均法，分为：简单平均法和加权平均法。
  2. 加权平均法的权重是每个个体学习器的权重。
  3. 在个体学习器性能相差较大时宜采用加权平均法，性能相近时宜采用简单平均法。
- 投票法
  1. 对于**分类预测问题**，通常使用的是投票法。
  2. **相对多数投票法**，也就是少数服从多数，即`T`个弱学习器的对样本$x$的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
  3. **绝对多数投票法**，也就是票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数，否则会拒绝预测。
  4. **加权投票法**，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。
- 学习法
  1. 代表方法是Stacking，当使用Stacking的结合策略时，不是对弱学习器的结果做简单的逻辑处理，而是**再加上一层学习器**。也就是说，将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
  2. 在这种情况下，将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。

</br>

#### Boosting方法

- 基于**串行策略**</br>
  基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。
- 基本思路
  1. 先从初始训练集训练一个基学习器；初始训练集中各样本的权重是相同的；
  2. 根据上一个基学习器的表现，**调整样本权重**，使分类错误的样本得到更多的关注；
  3. 基于调整后的样本分布，训练下一个基学习器；
  4. 测试时，对各基学习器加权得到最终结果。
- 特点是每次学习都会使用全部训练样本
- 代表算法包括AdaBoost、GBDT、XGBoost。
- 两个基本问题
  1. 每一轮如何改变数据的权值或概率分布？
  2. 如何将弱分类器组合成一个强分类器？

</br>

#### Bagging方法

- 基于**并行策略**</br>
  基学习器之间不存在依赖关系，可同时生成。
- 基本思路
  1. 利用自助采样法对训练集随机采样，重复进行`T`次；
  2. 基于每个采样集训练一个基学习器，并得到`T`个基学习器；
  3. 预测时，集体投票决策。
- 自助采样法：对`m`个样本的训练集，有放回的采样`m`次；此时，样本在`m`次采样中始终没被采样的概率约为`0.368`，即每次自助采样只能采样到全部样本的`63%`左右。
- 特点是训练每个基学习器时**只使用一部分样本**，并且偏好使用**不稳定的学习器**作为基学习器（所谓不稳定的学习器，指的是对样本分布较为敏感的学习器）。
- 代表算法包括随机森林。

</br>

#### Stacking方法简述

- 基于**串行策略**</br>
  初级学习器与次级学习器之间存在依赖关系，初级学习器的输出作为次级学习器的输入。
- 基本思路：
  1. 先从初始训练集训练`T`个不同的初级学习器；
  2. 利用每个初级学习器的输出构建一个次级数据集，该数据集依然使用初始数据集的标签；
  3. 据新的数据集训练次级学习器；
  4. 多级学习器的构建过程类似。
- Stacking方法也可以作为一种**结合策略**，比如加权平均和投票都属于结合策略。
- 特点是为了降低过拟合的风险，一般会利用**交叉验证**的方法使不同的初级学习器在不完全相同的子集上训练。

</br>

#### 为什么使用决策树作为基学习器

- 决策树的表达能力和泛化能力，可以通过**剪枝**（调节树的层次）快速调整。
- 决策树可以方便地将**样本的权重**整合到训练过程中，而不需要使用过采样的方式来调整样本权重（可以通过调整样本损失，或者影响分枝时样本的信息增益或基尼指数来实现）。
- 决策树是一种**不稳定**的学习器。所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。

</br>

#### 为什么不稳定的学习器更适合作为基学习器

- 不稳定的学习器**容易受到样本分布的影响**（方差大），很好的引入了**随机性**；这有助于在集成学习（特别是采用Bagging策略）中**提升模型的泛化能力**。
- 为了更好的引入随机性，有时会随机选择一个属性子集中的最优分裂属性，而不是全局最优（随机森林）。

> [!TIP|label:还有哪些模型也适合作为基学习器]
> **神经网络**也属于不稳定的学习器。通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。

</br>

#### 能否使用线性分类器作为基学习器

- Bagging 方法中不推荐
  1. 线性分类器都属于稳定的学习器（方差小），对数据不敏感；
  2. 甚至可能因为Bagging的采样，导致在训练中难以收敛，增大集成分类器的偏差。
- Boosting 方法中可以使用
  1. Boosting方法主要通过**降低偏差**的方式来提升模型的性能，而线性分类器本身具有方差小的特点；
  2. XGBoost中就支持以线性分类器作为基学习器。

</br>

#### 使用强分类器作为基学习器的弊端

- 第一次产生的分类器错误率已经达到了极限，后面的分类器起不到效果，根本原因是即使改变样本权重，**分类器的差异性依然太小**。
- 多个分类器基本处于相似位置的超平面，达不到组合变强的效果。

</br>

#### 偏差和方差

- 简单来说，Boosting能提升弱分类器性能的原因是**降低了偏差**，Bagging则是**降低了方差**。
- Boosting的基本思路就是在**不断减小模型的训练误差**（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差。但Boosting不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。
- Bagging方法对`n`个独立不相关的模型预测结果取平均，**方差是原来的`1/n`**。假设所有基分类器出错的概率是独立的，超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

</br>

#### AdaBoost如何解决Boosting的两个基本问题

- 每一轮如何改变数据的权值或概率分布？</br>
  开始时，每个样本的权值是一样的，AdaBoost的做法是**提高上一轮弱分类器错误分类样本的权值**，同时降低那些被正确分类样本的权值。
- 如何将弱分类器组合成一个强分类器？</br>
  AdaBoost**采取加权表决的方法**。具体的，AdaBoost会加大分类误差率小的基学习器的权值，使其在表决中起到更大的作用，同时减小分类误差率大的基学习器的权值。

</br>

#### AdaBoost算法

- 对于基学习器$G_1(x)$，初始化所有训练样本的权值分布为$D_1$
  $$
  D_1=\left(w_{11},w_{12},\ldots,w_{1N}\right),\ w_{1i}=\dfrac{1}{N},
  $$
  即每个样本具有**相同的权值**。
- 对于使用权值$D_m$得到的基分类器$G_m(x)$，计算其在训练集上的**分类误差率**，即被$G_m(x)$误分类样本的权值和
  $$
  e_m=P(G_m(x_i)\ne y_i)=\sum_{i=1}^{N}{w_{m,i}\cdot I(G_m(x_i)\neq y_i)}.
  $$
- 计算$G_m(x)$在最终加权表决中的权重
  $$
  \color{red} \alpha_m=\dfrac{1}{2}\ln{\dfrac{1-e_m}{e_m}}.
  $$
  由于$e_m$减小会使得$\alpha_m$增大，所以**分类误差率越小的基分类器在最终的最用越大**。
- 更新权值分布
  $$
  D_{m+1}=\left(w_{m+1,1},\ldots,w_{m+1,N}\right),
  $$
  $$
  w_{m+1,i}=\dfrac{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}{Z_m}=\dfrac{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}{\sum_{i=1}^{N}{w_{m,i}\cdot\exp{(-\alpha_m y_i G_m(x_i))}}}.
  $$
  其中$Z_m$作为一个形成概率分布的**规范化因子**。
- 所有基分类器的线性组合加权表决作为最终的分类器
  $$
  \color{red} G(x)={\rm sign}(f(x))={\rm sign}\left(\sum_{m=1}^{M}{\alpha_m G_m(x)}\right).
  $$
- 不改变训练数据，而**不断改变训练数据权值的分布**，使训练数据在基学习器的学习中起到不同的作用，这是 AdaBoost 的一个特点。

</br>

#### AdaBoost的优缺点

- 优点</br>
  能够基于泛化性能相当弱的的学习器构建出很强的集成，分类精度高，**不容易发生过拟合**。
- 缺点</br>
  **对异常样本比较敏感**，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。

</br>

#### 如何利用AdaBoost的权值

- 对于可以对特定数据分布进行学习的基学习器，可以通过**重赋权法**（re-weighting），例如可以**修改损失函数**，使得不同样本的损失值需要和权值相乘后生效。
- 对于无法接受带权样本的基学习算法，可以通过**重采样**（re-sampling）来产生训练集，这也是为什么之前说AdaBoost可以理解为基于概率分布来选取样本。详细做法是：10个样本中每个样本被抽中的概率是$w_{m,i}$，现在按抽取概率连续做10次可放回的样本抽取，得到训练集即可训练出一个分类器。

</br>

#### 为什么AdaBoost能快速收敛

每轮训练结束后，AdaBoost 会对样本的权重进行调整，调整的结果是越到后面被错误分类的样本权重会越高。而后面的分类器为了达到较低的带权分类误差，会把样本权重高的样本分类正确。这样造成的结果是，虽然每个弱分类器可能都有分错的样本，然而整个AdaBoost却能保证对每个样本进行正确分类，从而实现快速收敛。

</br>

#### 前向分步算法与AdaBoost

- AdaBoost算法是前向分步算法的特例。此时，基函数为基分类器，损失函数为指数函数$L(y,f(x))=\exp{(-y\ast f(x))}$。
Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而AdaBoost就是**加法模型+指数损失函数+前向分步算法**。

</br>

#### 提升树算法

- 提升树（Boosting Tree）以**决策树**为基学习器，对分类问题使用**二叉分类树**，回归问题使用**二叉回归树**。
- 提升树模型可表示为**决策树的加法模型**
  $$
  \color{red} f_M(x)=\sum_{m=1}^{M}T(x;\Theta_m).
  $$
- 根据前向分步算法，初始提升树$f_0(x)=0$，第$m$步的模型是
  $$
  f_m(x)=f_{m-1}(x)+T(x;\Theta_m).
  $$
- 通过**最小化损失函数**（经验风险极小化）决定当前决策树的参数
  $$
  {\hat{\Theta}}_m=\arg{\min_{\Theta_m}{\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))}}.
  $$
- 对于回归问题采用**平方误差损失函数**，对于分类问题采用**指数损失函数**。

</br>

#### 回归问题的提升树

- 根据决策树的相关知识，若将输入空间$X$划分为**多个互不相交的区域$R_j$**，并在每个区域给出固定输出$c_j$，则树可以表示为
  $$
  T(x;\Theta)=\sum_{j=1}^{J}{c_jI(x\in R_j)}.
  $$
- 在前向分步算法的第$m$步，根据当前模型$f_{m-1}(x)$求解第$m$棵树的参数${\hat{\Theta}}_m$。
- 若采用平方误差损失函数时
  $$
  L(y,f(x))=(y-f(x))^2,
  $$
  $$
  L(y,f_{m-1}(x)+T(x;\Theta_m))=(y-f_{m-1}(x)-T(x;\Theta_m))^2.
  $$
  其中$r=y-f_{m-1}(x)$是当前模型拟合数据的残差。所以，在回归问题中，新的树是**通过不断拟合残差得到的**。

</br>

#### GBDT算法

- 对于提升树来说，当损失函数为平方损失或指数损失时，每一步的优化是很直观的；但对于一般的损失函数而言，不太容易。梯度提升正是针对这一问题提出的算法。
- 梯度提升是梯度下降的近似方法，其关键是利用**损失函数的负梯度作为残差的近似值**，来训练新的基分类器，然后将训练好的基分类器以累加的形式结合。
- 初始化提升树，估计**使损失函数最小的常数值**，得到一棵只有一个根节点的树
  $$
  f_0(x)=\arg{\min_c{\sum_{i=1}^{N}L(y_i,c)}}.
  $$
- 对每一个样本**计算损失函数的负梯度作为残差的估计**（对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似）
  $$
  \color{red} r_{m,i}=-\dfrac{\partial L(y_i,f_{m-1}(x_i))}{\partial(f_{m-1}(x_i))},
  $$
  根据CART回归树，对$r_{m,i}$拟合一个回归树，得到第$m$棵树的叶子节点区域
  $$
  R_{m,j},\ j=1,2,\ldots,J.
  $$
- **计算每个叶子节点上的输出值**，即每个叶子节点上的输出值能使得节点内的样本的损失最小
  $$
  \color{red} c_{m,j}=\arg{\min_c{\sum_{x_i\in R_{m,j}} L(y_i,f_{m-1}(x_i)+c)}},
  $$
  更新当前回归树
  $$
  f_m(x)=f_{m-1}+\sum_{j=1}^{J}{c_{m,j}I(x\in R_{m,j})}.
  $$
- 最终得到的回归树表示为
  $$
  f_M(x)=\sum_{m=1}^{M}\sum_{j=1}^{J}{c_{m,j}I(x\in R_{m,j})}.
  $$

</br>

#### GBDT算法的一些特点

- GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是**把残差作为下一轮的学习目标**。
- 例如用GBDT预测年龄</br>
  `A`的真实年龄是`18`岁，但第一棵树的预测年龄是`12`岁，差了`6`岁，即残差为`6`岁。那么在第二棵树里把`A`的年龄设为`6`岁去学习，如果第二棵树真的能把`A`分到`6`岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是`5`岁，则`A`仍然存在`1`岁的残差，第三棵树里`A`的年龄就变成`1`岁，继续学。最终的结果由加权和值得到。
- 由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过`6`，而随机森林可以在`15`以上。
- GBDT与传统的Boosting区别较大，它的每一次计算都是**为了减少上一次的残差**。而为了消除残差，可以在残差减小的梯度方向上建立模型，所以说，在梯度提升中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方向，与传统的Boosting中关注正确错误的样本加权有着很大的区别。
- GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。

</br>

#### GBDT常用的损失函数

- 对于**分类问题**可以选用**指数损失函数、对数损失函数**。
- 对于**回归问题**可以选用**均方差损失函数、绝对损失函数**。
- 另外还有Huber损失函数和分位数损失函数，也是用于回归问题，可以增加回归问题的健壮性，可以减少异常点对损失函数的影响。

</br>

#### GBDT的正则化

- 在AdaBoost中会对每个模型乘上一个弱化系数（正则化系数），减小每个模型对提升的贡献（这个系数和模型的权重不一样，是在权重上又乘以一个小数）。
- 在GBDT中采用同样的策略，对于每个模型**乘以一个系数$**\lambda(0<\lambda<1)$，降低每个模型对损失的贡献，这种方法也意味着需要更多的基学习器。
- 第二种是每次通过按比例（推荐`[0.5, 0.8]`之间）**随机抽取部分样本来训练模型**，这种方法有点类似Bagging，可以减小方差，但同样会增加模型的偏差，可采用交叉验证选取，这种方式称为**子采样**。采用子采样的GBDT有时也称为随机梯度提升树（SGBT）。
- 第三种就是控制基学习器CART树的复杂度，可以采用剪枝。

</br>

#### GBDT的优点和局限性

- 优点
  1. **预测阶段**计算速度快，树与树之间**可并行化计算**。
  2. 在分布稠密的数据集上，**泛化能力**和**表达能力**都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
  3. 采用决策树作为弱分类器是的GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理归一化等。
- 局限性
  1. GBDT在**高维稀疏的数据集**上，表现不如支持向量机或者神经网络。
  2. GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
  3. **训练过程需要串行训练**，只能在决策树内部采用一些局部并行的手段提高训练速度。

</br>

#### XGBoost算法流程

- XGBoost的核心思想是不断地添加树，**不断地进行特征分裂来生长一棵树**，每次添加一个树，其实是学习一个新函数，去**拟合上次预测的残差**。
- 当训练完成得到`k`棵树，要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数。
- 最后只需要将每棵树对应的分数加起来就是该样本的预测值。

</br>

#### XGBoost算法

- 定义`K`棵**回归树**的集合$F=\{f(x)=w_{q(x)}\}$，其中$w_{q(x)}$指每个叶子节点的分数 。
- 算法的目标是使得树群的预测值${\hat{y}}_i$尽量接近真实值$y_i$，而且有尽量大的泛化能力。目标函数简化如下
  $$
  \color{red} L(\emptyset)=\sum_{i} l({\hat{y}}_i-y_i)+\sum_{k}\Omega(f_k).
  $$
  目标函数分为两个部分，包括**损失函数**（表示训练误差）和**正则化项**（定义复杂度）。一棵树的复杂度表示为
  $$
  \Omega(f)=\gamma T+\dfrac{1}{2}\lambda{||w||}^2,
  $$
  其中$T$表示叶子节点的个数。直观上看，目标要求预测误差尽量小，且叶子节点数量尽量少（$\gamma$控制叶子结点的个数），节点数值$w$尽量不极端（$\lambda$控制叶子节点的分数不会过大），**从而防止过拟合**。
- 对于训练的每一轮，需要选择一个新的树来使目标函数尽可能减小
  $$
  {\rm Obj}^{(t)}=\sum_{i=1}^{n}l\left(y_i,{\hat{y}}_i^{(t-1)}+f_t(x_i)\right)+\Omega(f_t)+{\rm constant}.
  $$
  其中，**第$t$轮模型的预测值${\hat{y}}_i^{(t)}$等于前$t-1$轮的预测值${\hat{y}}_i^{(t-1)}$加当前树$f_t(x_i)$**。当损失函数是平方误差时
  $$
  \begin{aligned}
  {\rm Obj}^{(t)}&=\sum_{i=1}^{n}\left(y_i-({\hat{y}}_i^{(t-1)}+f_t(x_i))\right)^2+\Omega(f_t)+{\rm constant}_1 \\
  &=\sum_{i=1}^{n}\left[2\left(\color{red}{\hat{y}}_i^{(t-1)}-y_i\right)f_t(x_i)+f_t(x_i)^2\right]+\Omega(f_t)+{\rm constant}_2.
  \end{aligned}
  $$
  标红部分即位残差。
- 但如果损失函数不是二次函数，需要利用**泰勒展开**将其近似为二次
  $$
  {\rm Obj}^{(t)}\simeq\sum_{i=1}^{n}\left[l\left(y_i,{\hat{y}}_i^{(t-1)}\right)+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t)+{\rm constant},
  $$
  其中$g_i=\partial_{{\hat{y}}_i^{(t-1)}}l\left(y_i,{\hat{y}}_i^{(t-1)}\right)$，$h_i=\partial_{{\hat{y}}_i^{(t-1)}}^2l\left(y_i,{\hat{y}}_i^{(t-1)}\right)$。移除所有常数项后
  $$
  {\rm Obj}^{(t)}=\sum_{i=1}^{n}\left[g_if_t(x_i)+\dfrac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t),
  $$
  第一项可以看作是每个样本在第$t$棵树的叶子节点得分值的结果之和。此时，目标函数只依赖于每个数据点在误差函数上的一阶导数$g_i$和二阶导数$h_i$。

</br>

#### GBDT和XGBoost的区别

- 传统的GBDT以CART树作为基学习器，XGBoost还支持**线性分类器**，这个时候XGBoost相当于**带`L1`和`L2`正则化的**逻辑斯蒂回归（分类）或者线性回归（回归）。
- 传统的GBDT在优化的时候只用到**一阶导数**信息，XGBoost则对代价函数进行了**二阶泰勒**展开，得到一阶和二阶导数。
- XGBoost在代价函数中加入了**正则项**，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，**防止过拟合**。
- **列抽样**，XGBoost借鉴了随机森林的做法，支持列抽样，能够防止过拟合和减少计算。
- 缺失值的处理，对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向。
- 特征维度上的并行化。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。

</br>

#### XGBoost为什么快

- XGBoost工具支持并行。它的并行不是树粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的。XGBoost的并行是在**特征粒度**上的，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- Shrinkage（缩减），相当于**学习速率**。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）。
- **可并行的近似直方图算法。**树节点在进行分裂时，需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以

</br>

#### 随机森林算法

- 根据Bagging算法，从原始训练集中使用自主采样方法**随机有放回采样取出`m`个样本**，共进行`T`次采样，生成`T`个训练集。对于每个训练集，单独训练决策树模型。
- 对于每棵树所对应的训练集，如果每个样本的特征维度为`M`，指定一个常数`m`，**随机地从`M`个特征中选取`m`个特征子集**，每次树进行节点分裂时，从这`m`个特征中选择最优的。
- 每棵树都已知这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝。
- 将生成的多颗决策树组成随机森林。对于分类问题，按照多棵树分类器**投票**决定最终分类结果；对于回归问题，由多颗树**预测值的均值**决定最终预测结果。

</br>

#### 随机森林的随机性体现在哪里

- **数据集的随机选取**</br>
  从原始的数据集中采取有放回的抽样（Bagging）来构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
- **待选特征的随机选取**</br>
  与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。若令$m=M$，则基决策树的构建与传统决策树相同；若令$m=1$，则是随机选择一个特征用于划分；一般情况下，推荐值$m=\log_2{M}$。

</br>

#### 随机森林的优缺点

- 优点
  1. 由于采用了集成算法，本身**精度**比大多数单个算法要好，所以准确性高；
  2. 在测试集上表现良好，由于两个随机性的引入，使得RF不容易陷入过拟合，也使得RF具有一定的**抗噪声能力**；
  3. 由于树的组合，使得RF可以处理**非线性数据**，本身属于非线性分类（拟合）模型；
  4. 它能够处理**很高维度的数据**，并且不用做特征选择，对数据集的适应能力强，即既能处理离散型数据，也能处理连续型数据，**数据集无需规范化**；
  5. 训练速度快，可以运用在大规模数据集上；
  6. 可以处理**缺省值**（单独作为一类），不用额外处理；
  7. 由于有**袋外数据**（OOB），可以在模型生成过程中取得真实误差的**无偏估计**，且不损失训练数据量；
  8. 在训练过程中，能够检测到特征间的互相影响，且可以得出特征的重要性，具有一定参考意义；
  9. 由于每棵树可以独立、同时生成，容易做成**并行化方法**。
- 缺点
  1. 当RF中的决策树个数很多时，训练时需要的空间和时间会比较大；
  2. RF中还有许多不好解释的地方，有点算是黑盒模型；
  3. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。

</br>

#### 随机森林中无偏估计

- 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在**生成的过程**中就可以对误差建立一个无偏估计。
- 在构建每棵树时，随机且有放回地抽取使得每棵树大约缺失了`1/3`的训练实例，它们称为第`k`棵树的OOB样本。而这样的采样特点就允许进行OOB估计，它的计算方式如下：
  1. 对每个样本，计算**将它作为OOB样本的树**对它的分类情况（约`1/3`的树）；
  2. 然后以简单多数投票作为该样本的分类结果；
  3. 最后用误分个数占样本总数的比率作为随机森林的OOB误分率。

</br>

> [!NOTE|label:参考资料]
> [集成学习](http://pelhans.com/2019/09/18/ML_mianshi-note12/?utm_source=tuicool&utm_medium=referral)</br>
> [ML-专题-集成学习](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/C-%E4%B8%93%E9%A2%98-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.md)</br>
> [提升树GBDT详解](https://blog.csdn.net/sb19931201/article/details/52506157)</br>
> [GBDT算法原理](https://blog.csdn.net/sangyongjia/article/details/83578718)</br>
> [通俗理解kaggle比赛大杀器xgboost](https://blog.csdn.net/v_july_v/article/details/81410574)</br>
> [机器学习--boosting家族之XGBoost算法](https://www.cnblogs.com/zongfa/p/9324684.html)</br>