# 神经网络

</br>

#### RNN简述

- RNN的层级展开图如下所示
  <div align=center><img src="./img/RNN.jpg" width=460/></div>
- 通常，其公式表达为
  $$
  \begin{aligned}
  h_t&=Ux_t+Ws_{t-1} \\
  s_t&=f(h_t) \\
  z_t&=Vs_t \\
  o_t&=g(z_t),
  \end{aligned}
  $$
  其中，$f$可以是`Tanh`、`ReLU`、`Sigmoid`等激活函数，$g$通常是`Softmax`。在每一个时刻，**权重矩阵都是共享的**。

> [!TIP|label:为什么需要RNN]
> 常规的神经网络模型只能单独地去处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。为了能够更好的处理序列的信息，RNN就诞生了。

</br>

#### RNN的反向传播

- 由于RNN在所有的时间步中共享参数$U, V, W$，因此每个输出的梯度不仅取决于当前时间步长的计算，而且还取决于以前的时间步长。例如，为了计算`t = 4`处的梯度，需要反向传播`3`个步骤并对梯度求和（链式法则），这叫做**时间反向传播（BPTT**）。
- 每一次的输出值$o_t$都会产生一个误差值。对于某一时间步`t = 3`，若此时的误差为$E_3$，**计算对$V$的梯度**
  $$
  \dfrac{\partial E_3}{\partial V}
  =\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}\dfrac{\partial z_3}{\partial V}
  =\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}s_3.
  $$
- **对$W$求梯度**
  $$
  \dfrac{\partial E_3}{\partial W}
  =\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}\dfrac{\partial z_3}{\partial s_3}\dfrac{\partial s_3}{\partial W}
  =\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}V\dfrac{\partial s_3}{\partial W},
  $$
  由于$s_3=f(Ux_3+Ws_2)$，因此$s_3$也和$s_2$有关，因此可以应用**链式法则**得到
  $$
  \begin{aligned}
  \dfrac{\partial s_3}{\partial W}
  &=\dfrac{\partial s_3}{\partial h_3}\dfrac{\partial h_3}{\partial W} \\
  &=\dfrac{\partial s_3}{\partial h_3}\color{red}(s_2+W\dfrac{\partial s_2}{\partial W}) \\
  &=\dfrac{\partial s_3}{\partial h_3}\left(s_2+W\left(\dfrac{\partial s_2}{\partial h_2}(s_1+W\dfrac{\partial s_1}{\partial W})\right)\right).
  \end{aligned}
  $$
- **对$U$求梯度**
  $$
  \begin{aligned}
  \dfrac{\partial E_3}{\partial U}
  =\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}\dfrac{\partial z_3}{\partial s_3}\dfrac{\partial s_3}{\partial U}
  ==\dfrac{\partial E_3}{\partial o_3}\dfrac{\partial o_3}{\partial z_3}V\dfrac{\partial s_3}{\partial U},
  \end{aligned}
  $$
  其中，根据链式法则
  $$
  \color{red} \dfrac{\partial s_t}{\partial U}=\dfrac{\partial s_3}{\partial h_3}(x_t+W\dfrac{\partial s_{t-1}}{\partial U}).
  $$
- 最终，将$\dfrac{\partial E_t}{\partial o_t}\dfrac{\partial o_t}{\partial z_t}=(o_t-y_t)s_t$代入，得到参数更新公式
  $$
  \begin{aligned}
  \dfrac{\partial E_t}{\partial V}&=(o_t-y_t)s_t \\
  \dfrac{\partial E_t}{\partial W}&=(o_t-y_t)V\dfrac{\partial s_t}{\partial h_t}(s_{t-1}+W\dfrac{\partial s_{t-1}}{\partial W}) \\
  \dfrac{\partial E_t}{\partial U}&=(o_t-y_t)V\dfrac{\partial s_t}{\partial h_t}(x_t+W\dfrac{\partial s_{t-1}}{\partial U}).
  \end{aligned}
  $$

</br>

#### RNN常见的设计模式

- 标准Elman RNN：每个时间步都有输出，且隐藏单元之间有循环连接。
- 忽视中间输出：隐藏单元之间有循环连接，但只有最后一个时间步有输出。
- Jordan RNN：每个时间步都有输出，但是隐藏单元之间没有循环连接，只有**当前时刻的输出**到**下个时刻的隐藏单元**之间有循环连接。模型表示能力更弱，但可以并行化训练。

</br>

#### RNN为什么会出现梯度消失

RNN梯度消失是因为激活函数`Tanh`函数的导数在`[0, 1]`（`Sigmoid`函数的导数在`[0, 0.25]`），反向传播时更新前面时刻的参数时，当参数`W`初始化为`小于1`的数，则多个导数和`W`的相乘（小于1的数连乘），**将导致求得的偏导极小**，从而导致梯度消失。

> [!TIP|label:如何解决RNN的梯度消失问题]
> 1. **选用`ReLU`（`LeakReLU`、`eLU`）激活函数。**`ReLU`函数的左侧导数为`0`，右侧导数恒为`1`，这就缓解了梯度消失的发生。此外，`ReLU`计算方便、计算速度快，可以加速网络的训练。缺点一是恒为`1`的导数容易导致梯度爆炸，但设定合适的阈值可以解决这个问题。缺点二是由于负数部分恒为`0`，会导致一些神经元无法激活（可通过设置小学习率部分解决）；
> 2. **加入Batch Normalization层**，其优点包括可加速收敛、提升训练稳定性、控制过拟合等。Batch Normalization通过对每一层的输出规范为均值和方差一致，消除了权重矩阵带来的放大缩小的影响；
> 3. **设计残差结构**；
> 4. **引入门控机制**，换用LSTM、GRU。

</br>

#### RNN为什么会出现梯度爆炸

RNN梯度爆炸是因为当参数初始化为足够大，使得`Tanh`函数的导数乘以`W`依然大于`1`（大于1的数连乘），则将导致**偏导极大**，从而导致梯度爆炸。

> [!TIP|label:如何解决RNN的梯度爆炸问题]
> 1. **梯度剪切**的思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内，这可以防止梯度爆炸；
> 2. **采用权重正则化**（`L1/L2`正则），通过对网络权重做正则限制过拟合，例如$\alpha{||W||}^2$。如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

</br>

#### RNN中能否使用`ReLU`作为激活函数

- `ReLU`最早就是为了解决RNN中的梯度消失问题而设计的。`ReLU`函数的改进就是它使得**导数非`0`即`1`**，这样的话只要一条路径上的导数都是`1`，无论神经网络是多少层，这一部分的乘积都始终为`1`，因此深层的梯度也可以传递到浅层中。
- 但使用`ReLU`也不能完全避免RNN中的梯度消失/爆炸问题，问题依然在于存在多个`W`的连乘项，**只要`W`不是单位矩阵**，还是可能会出现梯度消失/爆炸。
- 如果使用`ReLU`作为RNN的激活函数，RNN因为每一个时间步都共享参数的缘故，容易出现数值溢出问题
因此，推荐的做法是将W初始化为单位矩阵。

</br>

#### RNN相比ANN/CNN有什么特点

- ANN/CNN处理序列数据时存在的问题：
  1. 一般的前馈网络，通常接受一个定长向量作为输入，然后输出一个定长的表示；它需要一次性接收所有的输入，因而**忽略了序列中的顺序信息**；
  2. CNN在处理变长序列时，通过**滑动窗口+池化**的方式将输入转化为一个定长的向量表示，这样做可以捕捉到序列中的一些**局部特征**，但是**很难学习到序列间的长距离依赖**。
- RNN**处理时序数据**时的优势：
  1. RNN很适合处理序列数据，特别是带有时序关系的序列，比如文本数据；
  2. RNN把每一个时间步中的信息编码到状态变量中，使网络具有一定的记忆能力，从而更好的理解序列信息。
  3. 由于RNN具有对序列中时序信息的刻画能力，因此在处理序列数据时往往能得到更准确的结果。

</br>

#### LSTM简述

- LSTM的内部结构图如下所示
  <div align=center><img src="./img/LSTM.png" width=500/></div>
- 具体来说，LSTM中增加了三个门控机制（**遗忘门、输入门、输出门**）以及**内部记忆状态`C`**。
  1. **遗忘门`f`**控制前一步记忆状态中的信息有多大程度**被遗忘**
    <div align=center><img src="./img/LSTM遗忘门.png" width=500/></div>
  2. **输入门`i`**控制当前计算的新状态以多大的程度**更新到记忆状态中**
    <div align=center><img src="./img/LSTM输入门.png" width=500/></div>
  3. **记忆状态`C`**间的**状态转移**由输入门和遗忘门共同决定
    <div align=center><img src="./img/LSTM记忆状态.png" width=500/></div>
  4. **输出门`o`**控制当前的输出有多大程度取决于当前的记忆状态
    <div align=center><img src="./img/LSTM输出门.png" width=500/></div>

</br>

> [!NOTE|label:参考资料]
> [RNN梯度消失和爆炸](https://blog.csdn.net/jizhidexiaoming/article/details/81743584)</br>
> [DL-专题-RNN](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-深度学习/B-专题-RNN.md)</br>
> [Transformer、rnn与cnn三大特征提取器的比较](https://blog.csdn.net/ningyanggege/article/details/89707196)</br>
> [从Word Embedding到Bert模型](https://zhuanlan.zhihu.com/p/49271699)</br>