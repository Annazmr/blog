# 过拟合与正则化

</br>

#### 偏差与方差

- 偏差与方差分别是用于**衡量一个模型泛化误差**的两个方面：
  - 模型的**偏差**，指的是模型预测的**期望值**与**真实值**之间的差；
  - 模型的**方差**，指的是模型预测的**期望值**与**预测值**之间的差平方和。
- **偏差**用于描述模型的**拟合能力**，而**方差**用于描述模型的**稳定性**。

<div align=center><img src="./img/偏差和方差.png"/></div>
</br>

#### 导致偏差和方差的原因

- **偏差**通常是由于对学习算法做了**错误的假设**，或者**模型的复杂度不够**。
  - 比如真实模型是一个二次函数，而假设模型为一次函数，这就会导致偏差的增大（欠拟合）；
  - **由偏差引起的误差通常在训练误差上就能体现**，或者说训练误差主要是由偏差造成的。
- 方差通常是由于**模型的复杂度相对于训练集过高**导致的。
  - 比如真实模型是一个简单的二次函数，而假设模型是一个高次函数，这就会导致方差的增大（过拟合）；
  - **由方差引起的误差**通常体现在测试误差相对训练误差的**增量上**。

> [!TIP|label:深度学习中的偏差与方差]
> - 神经网络的拟合能力非常强，因此它的**训练误差（偏差）**通常较小；
> - 但是过强的拟合能力会导致较大的方差，使模型的**测试误差（泛化误差）**增大；
> - 因此深度学习的核心工作之一就是**研究如何降低模型的泛化误差**，这类方法统称为**正则化方法**。

</br>

#### 什么是欠拟合和过拟合

- 欠拟合指模型不能在训练集上获得**足够低的训练误差**，表现为输出结果的**高偏差**。
- 过拟合指模型的训练误差与测试误差（**泛化误差**）之间差距过大。反映在评价指标上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（**泛化能力差**），即输出结果的**高方差**。

</br>

#### 为什么会出现欠拟合和过拟合

- 欠拟合常常在**模型学习能力较弱**（模型复杂度过低），而**数据复杂度较高**的情况出现，此时模型由于学习能力不足，无法学习到数据集中的“一般规律”，因而导致泛化能力弱。
- 过拟合常常在**模型学习能力过强**的情况中出现，此时的模型学习能力太强，以至于将训练集单个样本自身的特点都能捕捉到，并将其认为是“一般规律”，同样这种情况也会导致模型泛化能力下降。

</br>

#### 有哪些降低欠拟合风险的方法
- **加入新的特征**，加入特征组合、高次特征，来增大假设空间。
- **添加多项式特征**，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。
- **减小正则化项的系数**，添加正则化项是为了限制模型的学习能力，减小正则化项的系数则可以放宽这个限制，使得模型更倾向于**更大的权重**来更好的拟合数据。
- **使用非线性模型**，比如核SVM 、决策树、深度学习等模型；或者**增加模型的复杂度**，例如增加神经网络的层数、神经元个数等。

</br>

#### 有哪些降低过拟合风险的方法
- **数据增强**，增加训练样本数量，或利用生成对抗网络（GAN）、机器翻译等生成新数据。
- **降低模型复杂度**，例如在决策树中降低树的深度或进行剪枝。
- **权值约束**（添加正则化项），`L1`或`L2`正则化。</br>
- **集成学习**，使用随机森林、GBDT等。
- **在神经网络中采用Dropout的方法。**
- **早停法**（Early Stopping），通过设定停止标准来结束训练。

</br>

#### `L0`、`L1`和`L2`正则化

- 正则化的作用实际上就是**防止模型过拟合，提高模型的泛化能力**。
- `LO`范数是指向量中**非0的元素的个数**。</br>
  如果用`L0`范数来规则化一个参数矩阵`W`的话，就是希望`W`的大部分元素都是`0`。换句话说，**让参数矩阵是稀疏的**。
- `L1`范数是指向量中**各个元素绝对值之和**。</br>
  参数值大小和模型复杂度是成正比的。**因此复杂的模型，其`L1`范数就大**，最终导致损失函数就大，说明这个模型就不够好。
- `L2`范数是向量元素**绝对值的平方和的平方根**，表示欧氏距离，也叫岭回归。</br>
  与`L1`范数不一样的是，它不会使`W`的每个元素为`0`，而只是接近于`0`。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。

> [!TIP|label:总结]
> `L1`会趋向于产生少量的特征，而其他的特征都是`0`，而`L2`会选择更多的特征，这些特征都会接近于`0`。所有特征中只有少数特征起重要作用的情况下，选择`L1`更合适；而如果所有特征中，大部分特征都能起作用，而且起的作用很平均，选择`L2`更合适。
> - `L1/L2`范数让模型变得稀疏，增加模型的可解析性，可用于特征选择。
> - `L2`范数让模型变得更简单，防止过拟合问题。

</br>

#### 实现参数的稀疏有什么好处

- **一个好处是可以简化模型，避免过拟合。**因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就不够理想了。
- 另一个好处是参数变少可以使整个模型获得**更好的可解释性**。

</br>

#### 为什么`L1`可以产生稀疏权值而`L2`不会

- 对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数`J`的最小值，带有`L1`范数和`L2`范数约束的二维图示如下
  <div align=center><img src="./img/L1和L2.png" width=350/></div>
- 图中`J`与`L`首次相交的点即是最优解。`L1`在和每个坐标轴相交的地方都**会有“顶点”出现**，多维的情况下，这些顶点会更多；**在顶点的位置就会产生稀疏的解**。而`J`与这些“顶点”相交的机会远大于其他点，因此`L1`正则化会产生稀疏的解。`L2`不会产生“顶点”，因此`J`与`L2`相交的点具有稀疏性的概率就会变得非常小。

</br>

#### Dropout策略

- 在前向传播的时候，Dropout可以让某个**神经元**的激活值以一定的概率`p`**停止工作**，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。
- 简单来说，Dropout通过参数共享提供了一种廉价的**Bagging集成**的近似，相当于集成了包括所有从基础网络除去部分单元后形成的子网络。
- 流程描述：
  1. 首先随机（临时）删掉网络中一部分的隐藏神经元，输入输出神经元保持不变；
  2. 然后把输入`x`通过修改后的网络前向传播，然后把得到的损失结果**通过修改的网络反向传播**。一小批训练样本执行完这个过程后，**在没有被删除的神经元上**更新对应的参数；
  3. 恢复被删掉的神经元，继续上述过程。

> [!NOTE|label:权重调整]
> - 依概率`p`屏蔽掉某些神经元，使其激活值为`0`以后，还需要对输出的激活向量$\{h_i\}$**进行缩放**（rescale），也就是乘以$\dfrac{p}{1-p}$。
> - 如果在训练的时候，经过置`0`后，没有对$\{h_i\}$进行缩放，那么**在测试的时候，就需要对权重进行缩放**。预测的时候，每一个单元的参数要预乘以`p`。比如一个神经元的输出是`x`，那么在训练的时候它有`p`的概率参与训练，`1-p`的概率丢弃，那么它输出的期望是$px+(1-p)0=px$，因此测试的时候把这个神经元的权重乘以`p`可以得到同样的期望。

</br>

#### 为什么Dropout可以解决过拟合

- **取平均的作用：**Dropout掉不同的隐藏神经元就类似在**训练不同的网络**，整个Dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
- **减少神经元之间复杂的共适应关系：**因为Dropout程序导致两个神经元不一定每次都在一个Dropout网络中出现，这样权值的更新不再依赖于**有固定关系的隐含节点的共同作用**，阻止了某些特征仅仅在其它特定特征下才有效果的情况，迫使网络去**学习更加鲁棒的特征**。从这个角度看Dropout就有点像`L1/L2`正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。

</br>

> [!NOTE|label:参考资料]
> [深度学习基础](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/A-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md)</br>
> [欠拟合和过拟合出现原因及解决方案](https://www.cnblogs.com/zhhfan/p/10476761.html)</br>
> [深度学习技巧之Early Stopping（早停法）](https://www.datalearner.com/blog/1051537860479157)</br>
> [一文弄懂L0、L1和L2正则化范式](https://blog.csdn.net/oTengYue/article/details/89644170)</br>
> [深度学习中Dropout原理解析](https://blog.csdn.net/program_developer/article/details/80737724)</br>
>