# 线性回归与逻辑回归

</br>

#### 线性回归

- 利用大量的样本$D={(x^{(i)},y^{(i)})}^N_{i=1}$，**通过有监督的学习，学习到由$x$到$y$的映射$f$**，利用该映射关系对未知的数据进行预估。因为`y`为连续值，所以是回归问题。
- 线性回归的假设函数
  $$
  f(x)=h_\theta(x)=\theta_0+\theta_1x_1+\ldots+\theta_mx_m,
  $$
  其向量形式为$f(x)=\theta^Tx$。
- 衡量参数的好坏利用均方误差（最小二乘）损失函数
  $$
  \color{red} J(\theta)=\dfrac{1}{2}\sum_{i=1}^N{\left(h_\theta(x^{(i)})-y^{(i)}\right)}^2.
  $$
- 通过**最小均方算法**（Least Mean Square，LMS），首先随便为$\theta$初始化一个值，然后改变$\theta$值让$J(\theta)$的取值变小，不断重复直至$J(\theta)$约等于最小值。迭代公式为
  $$
  \theta\coloneqq\theta-\alpha\dfrac{\partial}{\partial\theta}J(\theta).
  $$
  $J(\theta)$对$\theta$的偏导表示$J(\theta)$变化最大的方向。由于求的是极小值，因此梯度方向是偏导数的反方向。求解偏导的过程如下：
  $$
  \begin{aligned}
  \dfrac{\partial}{\partial\theta}J(\theta)&=\dfrac{\partial}{\partial\theta}\dfrac{1}{2}\sum_{i=1}^N{\left(h_\theta(x^{(i)})-y^{(i)}\right)}^2 \\
  &=\sum_{i=1}^N\left(h_\theta(x^{(i)})-y^{(i)}\right)\cdot\dfrac{\partial}{\partial\theta}\left(h_\theta(x^{(i)})-y^{(i)}\right) \\
  &=\sum_{i=1}^N\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}.
  \end{aligned}
  $$
  因此，迭代公式可以进一步改写为
  $$
  \color{red} \theta\coloneqq\theta-\alpha\sum_{i=1}^N\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}.
  $$
  同时，这也是**批梯度下降法**的计算公式。
- **随机梯度下降法**在计算下降最快的方向时时随机选一个数据进行计算，而不是扫描全部训练数据集，这样就加快了迭代速度。

</br>

#### 回归分析的基本假设

回归分析是一种**参数化方法**，即为了达到分析目的，需要设定一些“自然的”假设。如果目标数据集不满足这些假设，回归分析的结果就会出现偏差。若回归分析的假设函数为$Y=\theta_0+\theta_1x_1+\ldots+\theta_nx_n+\epsilon$，基本假设满足：
1. **线性性 & 可加性**</br>
   线性性：$x_1$每变动一个单位，$Y$相应变动$\theta_1$个单位，与$x_1$的绝对数值大小无关。</br>
   可加性：$x_1$对$Y$的影响是独立于其它自变量的。</br>
   若变量不能满足线性性和可加性，则模型将无法很好的描述变量之间的关系，极有可能导致**很大的泛化误差**。
2. **误差项$\epsilon$之间应相互独立**</br>
   若不满足这一特性，称模型具有**自相关性**，导致测得的**标准差偏小**，进而会导致**置信区间变窄**（经常发生于时间序列数据集上）。
3. **自变量之间应相互独立**</br>
   若不满足这一特性，称模型具有**多重共线性性**，导致测得的**标准差偏大**，**置信区间变宽**。
4. **误差项$\epsilon$应为常数**</br>
   若满足这一特性，称模型具有**同方差性**；若不满足，则为**异方差性**。</br>
   异方差性的出现意味着误差项的方差不恒定，这常常出现在有异常值的数据集上，如果使用标准的回归模型，这些异常值的重要性往往被高估。在这种情况下，标准差和置信区间不一定会变大还是变小。
5. **误差项$\epsilon$应呈正态分布**</br>
   如果误差项不呈正态分布，意味着置信区间会变得很不稳定，往往需要重点关注一些异常的点（误差较大但出现频率较高），来得到更好的模型。

</br>

#### 线性回归中的损失函数是怎么来的

- 假定$y^{(i)}$是样本$x^{(i)}$的标签，则**预测误差**记为
  $$
  \epsilon=y^{(i)}-h_\theta(x^{(i)}).
  $$
- 一个基本假设是，对于各个样本点来说，**$\epsilon$是独立同分布的**。根据独立同分布的中心极限定理，当样本点很多时，$\epsilon$应该服从均值为`0`、方差为$\sigma^2$的正态分布（高斯分布），因此可得
  $$
  p(\epsilon^{(i)})=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(\epsilon^{(i)})^2}{2\sigma^2}\right),
  $$
  可以改写为
  $$
  p(y^{(i)}|y^{(i)};\theta)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right).
  $$
- 由于**各个样本是独立的**，则它们的**联合概率密度**就是各自的概率密度的乘积。则似然函数
  $$
  L(\theta)=\prod_{i=1}^N{p(y^{(i)}|y^{(i)};\theta)},
  $$
  取对数可得
  $$
  \begin{aligned}
  \log{L(\theta)}&=\log\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\
  &=\sum_{i=1}^N\log\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right) \\
  &=N\log{\dfrac{1}{\sqrt{2\pi}\sigma}}-\dfrac{1}{\sigma^2}\dfrac{1}{2}\sum_{i=1}^N\left(y^{(i)}-\theta^Tx^{(i)}\right)^2.
  \end{aligned}
  $$
  因此，为了**使上述函数最大**，需要使$\dfrac{1}{2}\sum_{i=1}^N\left(y^{(i)}-\theta^Tx^{(i)}\right)^2$最小，就此得到了线性回归的损失函数。

</br>

#### 如何解决线性回归中的过拟合和欠拟合



</br>

> [!NOTE|label:参考资料]
> [线性回归和逻辑回归](https://blog.csdn.net/jiaoyangwm/article/details/81139362)</br>
> [线性回归、梯度下降](https://www.cnblogs.com/BYRans/p/4700202.html)</br>
> [回归分析的五个基本假设](https://blog.csdn.net/Noob_daniel/article/details/76087829)</br>
> [Linear Regression](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/Liner%20Regression/1.Liner%20Regression.md)</br>