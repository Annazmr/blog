# 决策树

</br>

#### 什么是决策树

决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：
1. 一棵树。
2. if-then规则的集合，该集合是决策树上的所有从根节点到叶节点的路径的集合。
3. 定义在**特征空间与类空间上的条件概率分布**。决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。

</br>

#### 决策树的基本思想

实际上就是寻找最纯净的划分方法，这个最纯净在数学上叫纯度，纯度通俗点理解就是目标变量要分得足够开（$y=1$的和$y=0$的混到一起就会不纯）。另一种理解是分类误差率的一种衡量。实际决策树算法往往用到的是不纯度。不纯度的选取有多种方法，每种方法也就形成了不同的决策树方法，比如`ID3`算法使用**信息增益**作为不纯度；`C4.5`算法使用**信息增益比**作为不纯度；`CART`（分类与回归树）算法使用**基尼系数**作为不纯度。

</br>

#### 优缺点

- 优点：
  1. 决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解。
  2. 决策树模型可以可视化，非常直观。
  3. 应用范围广，可用于分类和回归，而且非常容易做多类别的分类。
  4. 能够处理数值型和连续的样本特征。
  5. 分类速度快。
- 缺点：
  1. 很容易在训练数据中生成复杂的树结构，**造成过拟合**。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。
  2. 学习一棵最优的决策树被认为是NP-Complete问题。实际中的决策树是基于启发式的贪心算法建立的，这种算法不能保证建立全局最优的决策树。Random Forest 引入随机能缓解这个问题。

</br>

#### 训练的目标

- 决策树学习的本质上是从训练数据集中归纳出一组分类规则，使它与训练数据矛盾较小的同时具有较强的泛化能力。
- 决策树学习的损失函数通常是**正则化的极大似然函数**。
- 决策树学习的策略是以损失函数为目标函数的最小化。
- 由于这个最小化问题是一个NP完全问题，现实中，通常采用启发式算法（SMO算法）来近似求解这一最优化问题，得到的决策树是次最优的。

</br>

#### 训练的组成

- 决策树的训练通常由三部分组成：**特征选择、树的生成、剪枝**。
- 从总体上看，这三个步骤就是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这个过程就是划分特征空间，构建决策树的过程。

</br>

#### 特征选择的方法
特征选择在于选取对训练数据具有分类能力的特征，提高决策树的效率。通常的准则是**信息增益**或**信息增益比**。

</br>

#### ID3算法和信息增益

- ID3算法的核心是在决策树各个节点上应用**信息增益**准则选择特征。从根节点开始递归，对节点计算所有可能的特征的信息增益，选择信息增益**最大**的特征作为节点的特征，由该特征的不同取值建立子节点。
- 递归的终止条件通常有两个，一是所有训练数据子集被基本正确分类；二是没有合适的特征可选，即可用特征为`0`，或者可用特征的信息增益或信息增益比都很小了。
- 信息增益表示的是得知特征$X$的信息而使得类$Y$的不确定性减少的程度。
- 随机变量$X$的概率分布$P(X=x_i)=p_i$，其熵定义为
  $$
  H(X)=-\sum_{i=1}^{n}{p_i\log{p_i}}.
  $$
  熵是对随机变量不确定性的度量，也可以说是对随机变量的概率分布的一个衡量。熵越大，则随机变量的不确定性越大。对同一个随机变量，当它的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大。
- **条件熵**$H(Y|X)$表示在已知随机变量$X$的条件下，随机变量$Y$的不确定性，定义为
  $$
  H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}.
  $$
- **特征$A$对训练数据集$D$的信息增益是信息熵和条件熵的差**：$\color{red} g(D,A)=H(D)-H(D|A)$，其中
  $$
  \begin{aligned}
  &H(D)=-\sum_{k=1}^K{\dfrac{|C_k|}{|D|}\log{\dfrac{|C_k|}{|D|}}} \\
  &H(D|A)=\sum_{i=1}^n{\dfrac{|D_i|}{|D|}H(D_i)},
  \end{aligned}
  $$
  其中$C_k$表示数据集中类别为$k$的子集，$D_i$表示数据集中特征$A$取值为$a_i$的子集。

> [!NOTE|label:ID3的缺点]
> - 没有考虑连续特征；
> - 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题；
> - 对缺省值的情况没有考虑；
> - 没有考虑过拟合的问题。

</br>

#### C4.5和信息增益比

- 信息增益比也可能产生一个问题就是，**对可取数值数目较少的属性有所偏好**。因此，C4.5算法并不是直接选择使用信息增益比最大的候选划分属性，而是使用了一个启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。
- 特征A对训练数据集$D$的**信息增益比**是其信息增益$g(D,A)$与训练集关于特征$A$的熵之比
  $$
  \color{red} g_R(D,A)=\dfrac{g(D,A)}{H_A(D)},
  $$
  其中$H_A(D)=-\sum_{i=1}^{n}{\dfrac{|D_i|}{|D|}\log{\dfrac{|D_i|}{|D|}}}$，$n$是特征$A$取值的个数。

</br>

#### 损失函数

- 设树$T$的叶子节点个数为$|T|$，某一叶子节点$t$有$N_t$个样本点，其中$k$类的有$N_{tk}$个，则损失函数可以定义为：$\color{red} C_\alpha(T)=C(T)+\alpha|T|$，其中
  $$
  C(T)=\sum_{t=1}^{|T|}{N_tH_t(T)},
  $$
  $C(T)$表示模型对训练数据的预测误差，$|T|$表示模型复杂度。其中$t$上的经验熵
  $$
  H_t(T)=-\sum_{k}{\dfrac{N_{tk}}{N_t}\log{\dfrac{N_{tk}}{N_t}}},
  $$
- 参数$\alpha\ge0$控制**预测误差和模型复杂度之间的影响**，较大的$\alpha$促使选择较简单的树，较小的的$\alpha$促使选择较复杂的树，而$\alpha=0$意味着只考虑模型与训练数据的拟合程度。

</br>

#### 过拟合的原因和解决方法

- 对于过拟合现象产生的原因，有以下几个方面：
  1. 在决策树构建的过程中，对决策树的生长**没有进行合理的限制**（剪枝）；
  2. 在建模过程中使用了**较多的输出变量**，变量较多也容易产生过拟合；
  3. 样本中有一些**噪声数据**，噪声数据对决策树的构建的干扰很多，没有对噪声数据进行有效的剔除。
- 对于过拟合现象的预防措施，有以下一些方法：
  1. 选择合理的参数进行**剪枝**，一般用后剪枝的方法来做；
  2. K-folds**交叉验证**，将训练集分为K份，然后进行K次的交叉验证；
  3. **减少特征**，计算每一个特征和响应变量的相关性，常见的为皮尔逊相关系数，将相关性较小的变量剔除，当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等。

</br>

#### 剪枝

- 剪枝是决策树学习算法对付**过拟合**的主要手段。
- 决策树的剪枝往往通过极小化决策树整体的损失函数来实现，当$\alpha$确定时，选择损失函数最小的模型。
- 假设一组叶子节点回缩到其父节点之前与之后的决策树损失分别为$C_\alpha(T_A)$和$C_\alpha(T_B)$，若存在$\color{red} C_\alpha(T_A)\le C_\alpha(T_B)$，则进行剪枝。
- 剪枝算法可以由一种动态规划的算法实现。

</br>

#### 预剪枝和后剪枝

- **预剪枝**</br>
  在决策树生成过程中，对每个节点**在划分前先进行估计**，若当前的节点划分不能带来决策树泛化的提升（剪枝前和剪枝后精度的比较），则停止划分并将当前节点标记为叶子节点。
- **后剪枝**</br>
  先从训练数据集中生成一棵完整的决策树，然后**自底向上对非叶子节点进行考察**，若将该节点对应的子树替换为叶子结点能够带来决策树泛化能力的提升，则将该节点替换为叶子节点。
- 后剪枝和预剪枝的优缺点：</br>
  1. 后剪枝决策树通常比预剪枝决策树保留更多的分支；
  2. 一般情形下，后剪枝决策树的欠拟合风险小，泛化能力往往优于预剪枝；
  3. 后剪枝决策树训练开销比预剪枝大得多。

> [!TIP|label:Python中剪枝相关的参数]
> `max_depth`：树的高度</br>
> `min_samples_split`：叶子结点的数目</br>
> `max_leaf_nodes`：最大叶子节点数</br>
> `min_impurity_split`：限制不纯度

</br>

#### 如何处理缺失值

- 使用某一属性上**无缺失值的样本子集**来计算信息增益，并乘上无缺失值的样本比例。
- 每一个缺失该属性的样本会同时进入到所有分叉中，但样本权重从`1`调整为该分叉中**无缺失值的样本数在无缺失值的样本子集中所占的比例**。
- 如果测试样本中也存在缺失值，如果有专门处理缺失值的分支，就走这个分支；或者从属性最常用的分支走；也可以同时探查所有的分支，然后算每个类别的概率，取概率最大的类别赋值给该样本（C4.5）。

</br>

#### CART基本概念

- CART算法是在给定输入随机变量`X`条件下输出随机变量`Y`的**条件概率分布**的学习方法。
- CART算法假设决策树是**二叉树**，内部节点特征的取值为“是”和“否”。
- CART决策树等价于递归地二分每个特征，将输入空间或特征空间划分为**有限个单元**，然后在这些单元上确定在输入给定的条件下输出的条件概率分布。
- CART决策树既可以用于**分类**，也可以用于**回归**。
- CART对**回归树**用**平方误差最小化准则**来选择特征，对**分类树**用**基尼指数最小化准则**选择特征。

</br>

#### CART回归树

- 一个回归树对应着输入空间或特征空间的一个划分以及在划分单元上的输出值。
- 假设已将输入空间划分为$M$个单元$\{R_1,\ldots,R_M\}$，并在每个单元$R_m$上对应有输出值$c_m$，则该回归树可表示为
  $$
  f(x)=\sum_{m=1}^{M}{c_mI(x\in R_m)},
  $$
  其中$I$为指示函数。
- 如果已经划分好了输入空间，通常使用**平方误差**
  $$
  {\sum_{x_i\in R_m}\left(y_i-f(x_i)\right)}^2,
  $$
  作为损失函数来表示回归树对于训练数据的预测误差，通过最小化损失函数来求解每个划分单元的最优输出值。
- 如果使用平方误差，易知**最优输出值**即每个划分单元上所有实例的均值${\hat{c}}_m={\rm avg}(y_i|x_i\in R_m)$。

</br>

#### CART回归树如何划分输入空间

- 一个启发式方法是：**以特征向量中的某一个特征为标准进行切分**。
- 假设选择**特征向量中第$j$个特征**作为切分变量，选择**某个样本$x$在该特征上的某个值$x^{(j)}=s$作为切分点**（也可以是任意值$s$），定义两个划分$R_1(j,s)=\left\{x_i|x_i^{(j)}\le s\right\}$和$R_2(j,s)=\left\{x_i|x_i^{(j)}>s\right\}$。即根据某一特征$j$的某一取值$s$，将数据集划分成两部分，一部分样本的特征$j$都不大于$s$，另一部分则相反。
- 分别遍历$R_1$和$R_2$中的所有样本，计算它们标签的均值。$R_1$中所有样本的标签均值记为
  $$
  c_1={\rm avg}\left(y_i|x_i\in R_1(j,s)\right),
  $$
  $R_2$中的记为$c_2$。分别取$R_1$和$R_2$中所有样本标签与标签均值的**差值平方和**，作为取样本$x$作为切分点的衡量值
  $$
  \color{red} m\left(x^{(j)}=s\right)={\min_{c_1}{\sum_{x_i\in R_1}(y_i-c_1)}}^2+{\min_{c_2}{\sum_{x_i\in R_2}(y_i-c_2)}}^2.
  $$
- **遍历所有特征值$j$以及所有样本$x_i$**，计算$m_{ij}$，取最小的$m_{ij}$作为最优切分变量$j$和最优切分点$x_i$。

</br>

#### CART分类树

- 分类树用**基尼（Gini）指数**选择最优特征，同时决定该特征的最优二值切分点。
- 假设有$K$类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为
  $$
  {\rm Gini}(p)=\sum_{k=1}^{K}{p_k(1-p_k)}=1-\sum_{k=1}^{K}p_k^2.
  $$
- 对于给定的样本集合$D$，${\rm Gini}(D)$反映了**从数据集$D$中随机抽取两个样本，其类别标记不一致的概率**，因此${\rm Gini}(D)$越小，数据集纯度越高。该值计算为
  $$
  {\rm Gini}(D)=1-\sum_{k=1}^{K}\left(\dfrac{|C_k|}{|D|}\right)^2,
  $$
  其中$C_k$是$D$中属于第$k$类的样本子集。
- 如果训练数据集$D$根据某一特征$A$是否取值为$a$而被分割为$D_1$和$D_2$两个部分，则
  $$
  D_1=\{(x,y)\in D|A(x)=a\},\ D_2=D-D_1.
  $$
  则在特征$A$的条件下，$D$的基尼指数定义为
  $$
  \color{red} {\rm Gini}(D,A=a)=\dfrac{|D_1|}{|D|}{\rm Gini}(D_1)+\dfrac{|D_2|}{|D|}{\rm Gini}(D_2).
  $$
  ${\rm Gini}(D)$表示标准集合$D$的不确定性，而${\rm Gini}(D,A=a)$表示经过$A=a$分割后数据集$D$的不确定性。**基尼指数越大，样本的不确定性也就越大。**

</br>

#### CART分类树如何生成

- 从根节点开始递归，**对每一个特征的每一个可能的取值**，计算${\rm Gini}(D,A=a)$，并选取**基尼指数最小的一个组合$(A,a)$**作为最优特征和最优切分点，生成子节点，并将数据集依照特征分配到两个子节点中。
- 算法的停止条件是节点中的样本个数小于一个预设的阈值，或样本集的基尼指数小于一个预设的阈值（样本已基本属于同一类），或没有更多的特征。

</br>

#### 分类树和回归树的区别

- 启发函数不同，CART里分类树用**基尼指数**，回归树用**平方差**。
- 结点的值不同，分类树存储的是**最优分裂特征**，回归树存储的是**最佳切分点**。
- 回归树也可以解分类问题。

</br>

#### CART剪枝算法

- CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型更简单），从而防止过拟合。
- 相比一般剪枝算法，CART剪枝算法的优势在于，**不用提前确定$\alpha$值，而是在剪枝的同时找到最优的$\alpha$值**。
- 对于树内的每一个内部节点$t$，**自下而上**地计算剪枝后整体损失函数的减少程度
  $$
  g(t)=\dfrac{C(t)-C(T_t)}{|T_t|-1},
  $$
  且令$\alpha=\min{(\alpha,g(t))}$。选择$g(t)$最小的节点$t$进行剪枝，得到新树$T_1$，$T_1$为区间$[\alpha_1,\alpha_2)$间的最优子树。不断执行直到根节点。

</br>

#### 与逻辑回归的区别

- 对于有缺失值的数据，决策树可以应对，而逻辑回归要预先对缺失数据进行处理；
- 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归；（决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。）
- 逻辑回归擅长分析线性关系，而决策树对线性关系的把握较差。线性关系在实践中有很多优点：简洁，易理解，可以在一定程度上防止对数据的过度拟合。</br>
  逻辑回归应用的是样本数据线性可分的场景，输出结果是概率，即，输出结果和样本数据之间不存在直接的线性关系；</br>
  线性回归应用的是样本数据和输出结果之间存在线性关系的场景，即，自变量和因变量之间存在线性关系。
- 逻辑回归对极值比较敏感，容易受极端值的影响，而决策树在这方面表现较好。
- 应用上的区别：决策树的结果和逻辑回归相比略显粗糙。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。比如决策树确定17个节点，全部数据就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要去一定的训练和技巧。
- 执行速度上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。


</br>

> [!NOTE|label:参考资料]
> [决策树](https://www.jianshu.com/p/eee203a1ad11)</br>
> [机器学习算法](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95.md)</br>
> [决策树(一)——构造决策树方法](https://blog.csdn.net/u012328159/article/details/70184415)</br>
> [决策树(四)——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)</br>
> [逻辑回归与决策树在分类上的一些区别](https://blog.csdn.net/xmu_jupiter/article/details/47022823)</br>